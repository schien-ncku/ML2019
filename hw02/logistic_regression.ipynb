{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is based on the sample code provided by TA2019\n",
    "# at https://ntumlta2019.github.io/ml-web-hw2/LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_train = np.genfromtxt('X_train', delimiter=',', skip_header=1)\n",
    "# X_train = np.array(pd.read_csv(\"X_train\"), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 106)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.genfromtxt('Y_train', delimiter=',')\n",
    "# Y_train = np.array(pd.read_csv(\"Y_train\", header=None), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "# 0 age, 1 fnlwgt, 3 capital_gain, 4 capital_loss, 5 hours_per_week\n",
    "\n",
    "# col = [0,1,3,4,5,7,10,12,25,26,27,28] # this line by TA does not make sense\n",
    "col = [0,1,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# into _normalize_column_normal\n",
    "length = len(col)\n",
    "X_mean = np.reshape(np.mean(X_train[:,col],0), (1, length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.85816468e+01, 1.89778367e+05, 1.07764884e+03, 8.73038297e+01,\n",
       "        4.04374559e+01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = np.reshape(np.std(X_train[:,col],0), (1, length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.36402231e+01, 1.05548357e+05, 7.38517868e+03, 4.02954031e+02,\n",
       "        1.23472391e+01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,col] = np.divide(np.subtract(X_train[:,col],X_mean), X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03067056, -1.06361075,  1.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.83710898, -1.008707  ,  1.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.04264203,  0.2450785 ,  1.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 1.42360965, -0.35877741,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.21564337,  0.11095988,  1.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.98373415,  0.92989258,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of X_train normalization\n",
    "# this makes age, fnlwgt, capital_gain, capital_loss, hours_per_week data into normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step into the train(X_train, Y_train) function\n",
    "\n",
    "# first, split a validation set\n",
    "dev_size = 0.1155\n",
    "train_len = int(round(len(X_train)*(1-dev_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28800"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data = X_train[0:train_len] # use a different variable name to maintain source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28800, 106)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_data = Y_train[0:train_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28800,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = X_train[train_len:None]\n",
    "Y_dev = Y_train[train_len:None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3761, 106)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_data and Y_train data are training set\n",
    "\n",
    "# step 2, use 0+0*x1+0*x2+... for weight initialization\n",
    "w = np.zeros((X_train_data.shape[1],))\n",
    "b = np.zeros((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularize = True\n",
    "lamda = 0.001\n",
    "max_iter = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(Y_train_data)\n",
    "num_dev = len(Y_dev)\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_validation = []\n",
    "train_acc = []\n",
    "dev_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training (accuracy/loss):  0.8078472222222223 / 0.39656970562682065\n",
      "validation (accuracy/loss):  0.8035097048657271 / 0.05237679067953294\n",
      "training (accuracy/loss):  0.8167361111111111 / 0.3832923326813743\n",
      "validation (accuracy/loss):  0.8130816272268013 / 0.0506451441452001\n",
      "training (accuracy/loss):  0.8205902777777778 / 0.37626173154454196\n",
      "validation (accuracy/loss):  0.8183993618718426 / 0.04973278318142807\n",
      "training (accuracy/loss):  0.8239583333333333 / 0.3715708155535025\n",
      "validation (accuracy/loss):  0.8218558893911194 / 0.04912470904906834\n",
      "training (accuracy/loss):  0.8267361111111111 / 0.3680961658556751\n",
      "validation (accuracy/loss):  0.8245147567136399 / 0.04867424106897924\n",
      "training (accuracy/loss):  0.8280555555555555 / 0.3653622256325998\n",
      "validation (accuracy/loss):  0.8258441903749003 / 0.048319600346484276\n",
      "training (accuracy/loss):  0.8300347222222222 / 0.36312426232121425\n",
      "validation (accuracy/loss):  0.829034831161925 / 0.04802908654080315\n",
      "training (accuracy/loss):  0.8313541666666666 / 0.3612401990368679\n",
      "validation (accuracy/loss):  0.8311619250199415 / 0.04778432776204082\n",
      "training (accuracy/loss):  0.8329166666666666 / 0.3596204688947127\n",
      "validation (accuracy/loss):  0.8327572454134539 / 0.04757375338820006\n",
      "training (accuracy/loss):  0.83375 / 0.35820510862774174\n",
      "validation (accuracy/loss):  0.833289018877958 / 0.047389620945148016\n"
     ]
    }
   ],
   "source": [
    "# into the training epochs\n",
    "for epoch in range(max_iter):\n",
    "    # random shuffle for each epoch\n",
    "    # into the _shuffle(,)\n",
    "    randomize = np.arange(len(X_train_data))\n",
    "    np.random.shuffle(randomize)\n",
    "    X_train_data = X_train_data[randomize]\n",
    "    Y_train_data = Y_train_data[randomize]\n",
    "    #print(X_train_data.shape)\n",
    "    #print(Y_train_data.shape)\n",
    "    # done _shuffle(,)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    # Logistic regression train with batch\n",
    "    for idx in range(int(np.floor(len(Y_train_data)/batch_size))):\n",
    "        X = X_train_data[idx*batch_size:(idx+1)*batch_size]\n",
    "        Y = Y_train_data[idx*batch_size:(idx+1)*batch_size]\n",
    "            \n",
    "        # Find out the gradient of the loss\n",
    "        # w_grad, b_grad = _gradient_regularization(X, Y, w, b, lamda)\n",
    "        \n",
    "        # step into _gradient_regularization\n",
    "        # return the mean of the graident\n",
    "        # y_pred = get_prob(X, w, b)\n",
    "        \n",
    "        # step into get_prob\n",
    "        # the probability to output 1\n",
    "        # y_pred = _sigmoid(np.add(np.matmul(X_train_data, w), b))\n",
    "        \n",
    "        # step into _sigmoid\n",
    "        _temp_z = np.add(np.matmul(X_train_data, w), b)\n",
    "        y_pred = np.clip(1 / (1.0 + np.exp(-_temp_z)), 1e-6, 1-1e-6)\n",
    "        # end _sigmoid\n",
    "        \n",
    "        # end get_prob\n",
    "        \n",
    "        pred_error = Y_train_data - y_pred\n",
    "        w_grad = -np.mean(np.multiply(pred_error.T, X_train_data.T), 1)+lamda*w\n",
    "        b_grad = -np.mean(pred_error)\n",
    "        # end _gradient_regularization\n",
    "            \n",
    "        # gradient descent update\n",
    "        # learning rate decay with time\n",
    "        w = w - learning_rate/np.sqrt(step) * w_grad\n",
    "        b = b - learning_rate/np.sqrt(step) * b_grad\n",
    "        #print(b_grad)\n",
    "            \n",
    "        step = step+1\n",
    "        # end of for idx\n",
    "        \n",
    "    # Compute the loss and the accuracy of the training set\n",
    "    # y_train_pred = get_prob(X_train, w, b)\n",
    "    # step into get_prob... _sigmoid\n",
    "    _temp_z = np.add(np.matmul(X_train_data, w), b)\n",
    "    y_train_pred = np.clip(1 / (1.0 + np.exp(-_temp_z)), 1e-6, 1-1e-6)\n",
    "    # end get_prob\n",
    "    Y_train_pred = np.round(y_train_pred) # if >0.5 round up to 1\n",
    "    #print(y_train_pred)\n",
    "    #print(Y_train_pred)\n",
    "    \n",
    "    # train_acc.append(accuracy(Y_train_pred, Y_train_data))\n",
    "    # step into accuracy()\n",
    "    _acc = np.sum(Y_train_pred == Y_train_data)/len(Y_train_pred)\n",
    "    train_acc.append(_acc)\n",
    "    # end accuracy()\n",
    "    \n",
    "    # loss_train.append(_loss(y_train_pred, Y_train, lamda, w)/num_train)\n",
    "    # step into _loss()\n",
    "    # _cross_entropy(y_pred, Y_label) + lamda * np.sum(np.square(w))\n",
    "    # step into _cross_entropy\n",
    "    _cross_entropy = -np.dot(Y_train_data, np.log(y_train_pred))-np.dot((1-Y_train_data), np.log(1-y_train_pred))\n",
    "    # end _cross_entropy\n",
    "    _loss = _cross_entropy + lamda * np.sum(np.square(w))\n",
    "    # end _loss\n",
    "    loss_train.append(_loss/num_train)\n",
    "    \n",
    "    print(\"training (accuracy/loss): \", _acc, \"/\", _loss/num_train)\n",
    "    \n",
    "    # Compute the loss and the accuracy of the validation set\n",
    "    # y_dev_pred = get_prob(X_dev, w, b)\n",
    "    # Y_dev_pred = np.round(y_dev_pred)\n",
    "    # dev_acc.append(accuracy(Y_dev_pred, Y_dev))\n",
    "    # loss_validation.append(_loss(y_dev_pred, Y_dev, lamda, w)/num_dev)\n",
    "    _temp_z = np.add(np.matmul(X_dev, w), b)\n",
    "    y_dev_pred = np.clip(1 / (1.0 + np.exp(-_temp_z)), 1e-6, 1-1e-6)\n",
    "    Y_dev_pred = np.round(y_dev_pred) # if >0.5 round up to 1\n",
    "    _acc = np.sum(Y_dev_pred == Y_dev)/len(Y_dev_pred)\n",
    "    dev_acc.append(_acc)\n",
    "    _cross_entropy = -np.dot(Y_dev, np.log(y_dev_pred))-np.dot((1-Y_dev), np.log(1-y_dev_pred))\n",
    "    _loss = _cross_entropy + lamda * np.sum(np.square(w))\n",
    "    loss_validation.append(_loss/num_dev)\n",
    "    print(\"validation (accuracy/loss): \", _acc, \"/\", _loss/num_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
